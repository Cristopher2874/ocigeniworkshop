{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 3 Function Calling \n",
    "\n",
    "Thi is an example of Semantic Cache.  Semantic cache refrs to the idea of using semantic search to match on the key and return the value. This can be used by Tools like Rag to cache teh answers and return teh cached answer, fro all variations of the sqm questions\n",
    "\n",
    "steps: \n",
    "\n",
    "1. Setup :  Embed the questions and store the Q&A pairs\n",
    "2. R-etrival :  embed the question asked  & do similarity search\n",
    "3. A -ugmnet :  optionally rerank & add to response\n",
    "4. G - enerate : ask LLm to answer question based on the retrieved chunks\n",
    "\n",
    "Questions use #generative-ai-users  or #igiu-innovation-lab slack channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import OnDemandServingMode, EmbedTextDetails,CohereChatRequest, ChatDetails\n",
    "import oracledb\n",
    "import array\n",
    "import oci\n",
    "import os,json\n",
    "\n",
    "\n",
    "#####\n",
    "#make sure your sandbox.json file is setup for your environment. You might have to specify the full path depending on  your `cwd` \n",
    "#####\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.json\"\n",
    "\n",
    "EMBED_MODEL = \"cohere.embed-multilingual-v3.0\"\n",
    "LLM_MODEL = \"cohere.command-r-08-2024\" \n",
    "llm_service_endpoint= \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "tablename_prefix = None\n",
    "compartmentId = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunks we we want to query against "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The input text to vectorize\n",
    "qa_pairs = [\n",
    "    {\n",
    "        \"question\": \"What is the largest continent by land area?\",\n",
    "        \"answer\": \"Asia is the largest continent, covering about 30% of Earth's land area. It is home to diverse cultures, languages, and ecosystems.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which country has the longest coastline in the world?\",\n",
    "        \"answer\": \"Canada has the longest coastline, stretching over 202,080 kilometers. Its vast coastlines are along the Atlantic, Pacific, and Arctic Oceans.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What river is the longest in the world?\",\n",
    "        \"answer\": \"The Nile River is traditionally considered the longest river, flowing over 6,650 kilometers through northeastern Africa. It passes through countries like Egypt and Sudan.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which desert is the largest hot desert in the world?\",\n",
    "        \"answer\": \"The Sahara Desert is the largest hot desert, covering approximately 9.2 million square kilometers. It spans across North Africa from the Atlantic Ocean to the Red Sea.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the smallest country in the world by land area?\",\n",
    "        \"answer\": \"Vatican City is the smallest country, with an area of just 44 hectares (110 acres). It serves as the spiritual and administrative center of the Roman Catholic Church.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which mountain is the highest in the world above sea level?\",\n",
    "        \"answer\": \"Mount Everest is the highest mountain above sea level, standing at 8,848 meters (29,029 feet). It is part of the Himalayas and located on the border between Nepal and China.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What ocean is the deepest in the world?\",\n",
    "        \"answer\": \"The Pacific Ocean is the deepest ocean, with an average depth of about 4,280 meters. The Mariana Trench within it reaches depths of over 10,900 meters.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which two continents are entirely located in the Southern Hemisphere?\",\n",
    "        \"answer\": \"Australia and Antarctica are entirely located in the Southern Hemisphere. Both continents have unique ecosystems and climates.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What country has the most time zones?\",\n",
    "        \"answer\": \"France has the most time zones when including its overseas territories. In total, it spans 12 different time zones across various regions worldwide.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which lake is considered the world's largest by surface area?\",\n",
    "        \"answer\": \"Lake Superior, part of North America's Great Lakes, is often considered the largest freshwater lake by surface area. It covers approximately 82,100 square kilometers.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scfg = None\n",
    "# read the sandbox config \n",
    "with open(os.path.expanduser(SANDBOX_CONFIG_FILE), 'r') as f:\n",
    "                scfg=  json.load(f)\n",
    "                \n",
    "config = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]),scfg[\"oci\"][\"profile\"])\n",
    "compartmentId = scfg[\"oci\"][\"compartment\"]\n",
    "tablename_prefix = scfg[\"db\"][\"tablePrefix\"]\n",
    "wallet = os.path.expanduser(scfg[\"db\"][\"walletPath\"])\n",
    "                \n",
    "db = oracledb.connect(  config_dir=scfg[\"db\"][\"walletPath\"],user= scfg[\"db\"][\"username\"], password=scfg[\"db\"][\"password\"], dsn=scfg[\"db\"][\"dsn\"],wallet_location=scfg[\"db\"][\"walletPath\"],wallet_password=scfg[\"db\"][\"walletPass\"])\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = [\n",
    "\tf\"\"\"drop table if exists {tablename_prefix}_semantic_cache purge\"\"\"\t,\n",
    " \n",
    " \n",
    "\tf\"\"\"\n",
    "\t\tcreate table {tablename_prefix}_semantic_cache (\n",
    "\t\tid number,\n",
    "\t\tquestion varchar2(4000),\n",
    "\t\tanswer varchar2(4000),\n",
    "\t\tembedding vector,\n",
    "\t\tprimary key (id)\n",
    "\t)\"\"\"\n",
    "\t]\n",
    " \n",
    "for s in sql : \n",
    "\tcursor.execute(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up LLM client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a llm client \n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "\t\t\t\tconfig=config, \n",
    "\t\t\t\tservice_endpoint=llm_service_endpoint, \n",
    "\t\t\t\tretry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "\t\t\t\ttimeout=(10,240))\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_text_detail = EmbedTextDetails()\n",
    "embed_text_detail.serving_mode = OnDemandServingMode(model_id=\"cohere.embed-english-v3.0\")\n",
    "embed_text_detail.truncate = embed_text_detail.TRUNCATE_END\n",
    "embed_text_detail.input_type = embed_text_detail.INPUT_TYPE_SEARCH_DOCUMENT\n",
    "embed_text_detail.compartment_id = compartmentId\n",
    "embed_text_detail.inputs = [pair[\"question\"] for pair in qa_pairs] \n",
    "\n",
    "response = llm_client.embed_text(embed_text_detail)\n",
    "embeddings = response.data.embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insert embedding in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(embeddings)):\n",
    "    cursor.execute(f\"insert into {tablename_prefix}_semantic_cache values (:1, :2, :3, :4)\", \n",
    "                   [i,qa_pairs[i]['question'],qa_pairs[i]['answer'], array.array(\"f\",embeddings[i])])\n",
    "    print(f\"inserted {i}-{qa_pairs[i]}\")\n",
    "\n",
    "print(\"commiting\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"select id,question,answer from {tablename_prefix}_semantic_cache\")\n",
    "for row in cursor:\n",
    "\tprint(f\"{row[0]}:{row[1]}:{[row[2]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask A question to answer\n",
    "\n",
    "ask questions similar to questions above.  change the wording and see if the semantic cahce returns teh right answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Ask a question: \").strip().lower()\n",
    "q=[]\n",
    "q.append(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embed the query\n",
    "\n",
    "we nede to do the \"R\" part of rag - retrieve.  we retrieve in following steps\n",
    "1. embed the query test\n",
    "1. do a similarity serach to find the text similar to it \n",
    "2. optionally rerank it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embed\n",
    "\n",
    "embed_text_detail.inputs = q\n",
    "embed_text_detail.input_type = EmbedTextDetails.INPUT_TYPE_SEARCH_QUERY\n",
    "\n",
    "response = llm_client.embed_text(embed_text_detail)\n",
    "vec = array.array(\"f\",response.data.embeddings[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simialrity search of embedded text \n",
    "\n",
    " # There are multiple search algorithms: COSINE, DOT, EUCLIDEANN, try different algos\n",
    " # try adding the constraint the distance of < 0.5 is something we will need to finetune based on data\n",
    "cursor.execute(f\"\"\"\n",
    "\tselect id,question,answer, vector_distance(embedding, :1, COSINE) d \n",
    "\tfrom {tablename_prefix}_semantic_cache\n",
    "\torder by d\n",
    "\tfetch first 10 rows only\n",
    "\t\"\"\", [vec])\n",
    "\n",
    "rows =[]\n",
    "for row in cursor:\n",
    "\tr = [row[0], row[1], row[2], row[3]]\n",
    "\tprint(r)\n",
    "\trows.append(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optionally rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at cohere reranking example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print the response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"**************************Chat Result**************************\")\n",
    "print (f\"Answer is {rows[0][2]}\")\n",
    "print (\"\\n other answers:\\n\")\n",
    "for chunk in rows[0:3]: \n",
    "\tprint(f\"{chunk[3]}:{chunk[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## close the database connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "db.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Add semantic cache to your Rag agent and see the performance difference\n",
    "1. check to see if the question is already answered\n",
    "   * decide on the treshold of similar distance\n",
    "2.  If its under the distance return the matching answer\n",
    "3.  if not, ask the question to the agent\n",
    "    *  store the answer\n",
    "4.  tray asking a different worded questiona nd se eif it hits the cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
