{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 3 : Agents\n",
    "\n",
    "## Single Step  Tool function calling\n",
    "\n",
    "### Supported models (https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm) \n",
    "- cohere.command-r-08-2024\n",
    "- cohere.command-r-16k\n",
    "- cohere.command-r-plus\n",
    "- cohere.command-r-plus-08-2024\n",
    "\n",
    "\n",
    "Questions use #generative-ai-users  or #igiu-innovation-lab slack channel\n",
    "\n",
    "if you have errors running sample code reach out for help in #igiu-ai-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the  variables\n",
    "\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import OnDemandServingMode, EmbedTextDetails,CohereChatRequest, ChatDetails\n",
    "import oci\n",
    "import json, os\n",
    "\n",
    "#####\n",
    "#make sure your sandbox.json file is setup for your environment. You might have to specify the full path depending on  your `cwd` \n",
    "# you can also try making your cwd ofr jupyter match your workspace python code: \n",
    "# vscopde menu -> Settings > Extensions > Jupyter > Notebook File Root\n",
    "# change from ${fileDirname} to ${workspaceFolder}\n",
    "#####\n",
    "\n",
    "#SANDBOX_CONFIG_FILE = \"~/work/code/python/workshop/sandbox.json\"\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.json\"\n",
    "\n",
    "LLM_MODEL = \"cohere.command-r-16k\" \n",
    "PREAMBLE = \"\"\"\n",
    "        Analyze the problem and pick the right set of tools to answer the question\n",
    "\"\"\"\n",
    "MESSAGE = \"\"\"\n",
    "       \"I'd like 4 apples and a fish please\"\n",
    "\"\"\"\n",
    "llm_service_endpoint= \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sep up the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_param = oci.generative_ai_inference.models.CohereParameterDefinition()\n",
    "item_param.description = \"the item requested to be purchased, in all caps eg. Bananas should be BANANAS\"\n",
    "item_param.type = \"str\"\n",
    "item_param.is_required = True\n",
    "\n",
    "quantity_param = oci.generative_ai_inference.models.CohereParameterDefinition()\n",
    "quantity_param.description = \"how many of the items should be purchased\"\n",
    "quantity_param.type = \"int\"\n",
    "quantity_param.is_required = True\n",
    "\n",
    "shop_tool = oci.generative_ai_inference.models.CohereTool()\n",
    "shop_tool.name = \"personal_shopper\"\n",
    "shop_tool.description = \"Returns items and requested volumes to purchase\"\n",
    "shop_tool.parameter_definitions = {\n",
    "    \"item\": item_param,\n",
    "    \"quantity\": quantity_param\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the config file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scfg = None\n",
    "# read the sandbox config \n",
    "with open(os.path.expanduser(SANDBOX_CONFIG_FILE), 'r') as f:\n",
    "                scfg=  json.load(f)\n",
    "\n",
    "#read the oci config\n",
    "config = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]),scfg[\"oci\"][\"profile\"])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## specify the tools to use in the chat request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat request      \n",
    "llm_chat_request = CohereChatRequest()\n",
    "llm_chat_request.preamble_override = PREAMBLE \n",
    "llm_chat_request.message = MESSAGE\n",
    "llm_chat_request.is_stream = False \n",
    "llm_chat_request.max_tokens = 500 # max token to generate, can lead to incomplete responses\n",
    "llm_chat_request.is_force_single_step = True\n",
    "llm_chat_request.tools = [ shop_tool]\n",
    "\n",
    "\n",
    "\n",
    "# set up chat details\n",
    "chat_detail = ChatDetails()\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=LLM_MODEL)\n",
    "chat_detail.compartment_id = scfg[\"oci\"][\"compartment\"]\n",
    "chat_detail.chat_request = llm_chat_request\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## call the LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the LLM client \n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "                config=config,\n",
    "                service_endpoint=llm_service_endpoint,\n",
    "                retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "                timeout=(10,240))\n",
    "\n",
    "step = 1\n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "print(f\"**************************Step {step} Result**************************\")\n",
    "print(f\"message = {chat_response.data.chat_response.text}\")\n",
    "print(f\"tool calls = {chat_response.data.chat_response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the tool\n",
    "\n",
    "Note: \n",
    "1. in this example we are not explicity calling the tool, we are just returning a made up response.  you will insert an explicit call to teh toolapi for real code\n",
    "2. We have the call teh llm again with teh tool response for the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat_request.tool_results = []\n",
    "i = 0\n",
    "for call in chat_response.data.chat_response.tool_calls:\n",
    "        tool_result = oci.generative_ai_inference.models.CohereToolResult()\n",
    "        tool_result.call = call\n",
    "        # try to change response to out of stock etc for one or both items and see\n",
    "        tool_result.outputs = [ { \"response\": \"Completed, in stock\" } ] \n",
    "        llm_chat_request.tool_results.append(tool_result)\n",
    "\n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "\n",
    "# Print result\n",
    "print(\"**************************Step 2 Result**************************\")\n",
    "print(f\"message = {chat_response.data.chat_response.text}\")\n",
    "print(f\"tool calls = {chat_response.data.chat_response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming version \n",
    "\n",
    "Steaming response reduces latency, specially if response has a lot of text. but its involved as we have to process events \n",
    "\n",
    "we first define the function to process the events\n",
    "\n",
    "\n",
    "PS: this seems to be broken. refer to python code ( command_r_tool_single_step_demo_streaming.py) for the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tool_calls(chat_response):\n",
    "    for event in chat_response.data.events():\n",
    "        res = json.loads(event.data)\n",
    "        text = res['text']\n",
    "        if 'finishReason' in res:\n",
    "            if 'toolCalls' in res:\n",
    "                #print(f\"\\ntools to use : {res['toolCalls']}\",flush=True)\n",
    "                return text,res['toolCalls']\n",
    "            else:\n",
    "                return text,None\n",
    "        else:\n",
    "            if 'text' in res:\n",
    "                print(res['text'], end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "    return None,None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### call the llm in streaming mode \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat_request.is_stream = True\n",
    "llm_chat_request.tool_results = None\n",
    "step =1 \n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "\n",
    "text,tool_calls= get_tool_calls(chat_response)\n",
    "print(f\"\\n **************************Step {step} Result**************************\")\n",
    "print(f\"message = {text}\")\n",
    "print(f\"tool calls = {tool_calls}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# call tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat_request.tool_results = []\n",
    "for call in tool_calls:\n",
    "    tool_result = oci.generative_ai_inference.models.CohereToolResult()\n",
    "    tool_result.call = call\n",
    "    tool_result.outputs = [ { \"response\": \"Completed\" } ] \n",
    "    llm_chat_request.tool_results.append(tool_result)\n",
    "\n",
    "step = step+1\n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "text,tool_calls= get_tool_calls(chat_response)\n",
    "print(f\"message = {text}\")\n",
    "print(f\"tool calls = {tool_calls}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise : Weather Answering App\n",
    "\n",
    "1. Create an App that  gives the weather information for a given city \n",
    "    * Eg:  \tA. Lucknow will have temp around 25 degrees and will rain \n",
    "\n",
    "2. calls the weather API for a given city\n",
    "    * Api hosted at : https://ic-edge.ugbu.oraclepdemos.com/ash/docs\n",
    "        * Input \n",
    "            * City Name  \n",
    "            * DatDays in future\n",
    "        * Output\n",
    "            * Low\n",
    "            * High\n",
    "            * Chance of rain \n",
    "3. Types of questions supported\n",
    "    * What is the weather in lucknow tomm\n",
    "    * What will be highs in capital of india\n",
    "    * Which major city in Karnataka will it rain tomm \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
