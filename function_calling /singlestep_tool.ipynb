{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 3 : Agents\n",
    "\n",
    "## single Step  Tool function calling\n",
    "\n",
    "### Supported models (https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm) \n",
    "- cohere.command-r-08-2024\n",
    "- cohere.command-r-16k\n",
    "- cohere.command-r-plus\n",
    "- cohere.command-r-plus-08-2024\n",
    "\n",
    "\n",
    "Questions use #generative-ai-users  or #igiu-innovation-lab slack channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the  variables\n",
    "\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import OnDemandServingMode, EmbedTextDetails,CohereChatRequest, ChatDetails\n",
    "import oci\n",
    "import json\n",
    "\n",
    "CONFIG_PROFILE = \"AISANDBOX\"\n",
    "compartmentId= \"ocid1.compartment.oc1..aaaaaaaaxj6fuodcmai6n6z5yyqif6a36ewfmmovn42red37ml3wxlehjmga\" \n",
    "\n",
    "LLM_MODEL = \"cohere.command-r-16k\" \n",
    "PREAMBLE = \"\"\"\n",
    "        Analyze the problem and pick the right set of tools to answer the question\n",
    "\"\"\"\n",
    "MESSAGE = \"\"\"\n",
    "       \"I'd like 4 apples and a fish please\"\n",
    "\"\"\"\n",
    "llm_service_endpoint= \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sep up the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_param = oci.generative_ai_inference.models.CohereParameterDefinition()\n",
    "item_param.description = \"the item requested to be purchased, in all caps eg. Bananas should be BANANAS\"\n",
    "item_param.type = \"str\"\n",
    "item_param.is_required = True\n",
    "\n",
    "quantity_param = oci.generative_ai_inference.models.CohereParameterDefinition()\n",
    "quantity_param.description = \"how many of the items should be purchased\"\n",
    "quantity_param.type = \"int\"\n",
    "quantity_param.is_required = True\n",
    "\n",
    "shop_tool = oci.generative_ai_inference.models.CohereTool()\n",
    "shop_tool.name = \"personal_shopper\"\n",
    "shop_tool.description = \"Returns items and requested volumes to purchase\"\n",
    "shop_tool.parameter_definitions = {\n",
    "    \"item\": item_param,\n",
    "    \"quantity\": quantity_param\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## specify the tools to use in the chat request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oci key enabled for api access\n",
    "config = oci.config.from_file('~/.oci/config', CONFIG_PROFILE)\n",
    "\n",
    "# chat request      \n",
    "llm_chat_request = CohereChatRequest()\n",
    "llm_chat_request.preamble_override = PREAMBLE \n",
    "llm_chat_request.message = MESSAGE\n",
    "llm_chat_request.is_stream = False \n",
    "llm_chat_request.max_tokens = 500 # max token to generate, can lead to incomplete responses\n",
    "llm_chat_request.is_force_single_step = True\n",
    "llm_chat_request.tools = [ shop_tool]\n",
    "\n",
    "\n",
    "\n",
    "# set up chat details\n",
    "chat_detail = ChatDetails()\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=LLM_MODEL)\n",
    "chat_detail.compartment_id = compartmentId\n",
    "chat_detail.chat_request = llm_chat_request\n",
    "\n",
    "# set up the LLM client \n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "                config=config,\n",
    "                service_endpoint=llm_service_endpoint,\n",
    "                retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "                timeout=(10,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## call the LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Step 1 Result**************************\n",
      "message = \n",
      "tool calls = [{\n",
      "  \"name\": \"personal_shopper\",\n",
      "  \"parameters\": {\n",
      "    \"item\": \"APPLES\",\n",
      "    \"quantity\": 4\n",
      "  }\n",
      "}, {\n",
      "  \"name\": \"personal_shopper\",\n",
      "  \"parameters\": {\n",
      "    \"item\": \"FISH\",\n",
      "    \"quantity\": 1\n",
      "  }\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "print(f\"**************************Step {step} Result**************************\")\n",
    "print(f\"message = {chat_response.data.chat_response.text}\")\n",
    "print(f\"tool calls = {chat_response.data.chat_response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the tool\n",
    "\n",
    "Note: \n",
    "1. in this example we are not explicity calling the tool, we are just returning a made up response.  you will insert an explicit call to teh toolapi for real code\n",
    "2. We have the call teh llm again with teh tool response for the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Step 2 Result**************************\n",
      "message = That's no problem! I've checked and we have both apples and fish in stock. Can I help you with anything else?\n",
      "tool calls = None\n"
     ]
    }
   ],
   "source": [
    "llm_chat_request.tool_results = []\n",
    "i = 0\n",
    "for call in chat_response.data.chat_response.tool_calls:\n",
    "        tool_result = oci.generative_ai_inference.models.CohereToolResult()\n",
    "        tool_result.call = call\n",
    "        # try to change response to out of stock etc for one or both items and see\n",
    "        tool_result.outputs = [ { \"response\": \"Completed, in stock\" } ] \n",
    "        llm_chat_request.tool_results.append(tool_result)\n",
    "\n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "\n",
    "# Print result\n",
    "print(\"**************************Step 2 Result**************************\")\n",
    "print(f\"message = {chat_response.data.chat_response.text}\")\n",
    "print(f\"tool calls = {chat_response.data.chat_response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming version \n",
    "\n",
    "Steaming response reduces latency, specially if response has a lot of text. but its involved as we have to process events \n",
    "\n",
    "we first define the function to process the events\n",
    "\n",
    "\n",
    "PS: this seems to be broken. refer to python code ( command_r_tool_single_step_demo_streaming.py) for the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tool_calls(chat_response):\n",
    "    for event in chat_response.data.events():\n",
    "        res = json.loads(event.data)\n",
    "        text = res['text']\n",
    "        if 'finishReason' in res:\n",
    "            if 'toolCalls' in res:\n",
    "                #print(f\"\\ntools to use : {res['toolCalls']}\",flush=True)\n",
    "                return text,res['toolCalls']\n",
    "            else:\n",
    "                return text,None\n",
    "        else:\n",
    "            if 'text' in res:\n",
    "                print(res['text'], end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "    return None,None\n",
    "\n",
    "\n",
    "def get_tool_calls(chat_response):\n",
    "    for event in chat_response.data.events():\n",
    "        res = json.loads(event.data)\n",
    "        if 'finishReason' in res:\n",
    "            print(f\"\\ntools to call: {res['toolCalls']}\")\n",
    "            return res['toolCalls']\n",
    "        if 'text' in res:\n",
    "            print(res['text'], end=\"\", flush=True)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### call the llm in streaming mode \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's no problem! I've checked and we have both apples and fish in stock. I'll grab you 4 apples and 1 fish.\n",
      "\n",
      "Can I help you with anything else?"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'toolCalls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m step \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \n\u001b[1;32m      3\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mchat(chat_detail)\n\u001b[0;32m----> 5\u001b[0m text,tool_calls\u001b[38;5;241m=\u001b[39m get_tool_calls(chat_response)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m **************************Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Result**************************\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mget_tool_calls\u001b[0;34m(chat_response)\u001b[0m\n\u001b[1;32m     20\u001b[0m res \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(event\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinishReason\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtools to call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoolCalls\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoolCalls\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m res:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'toolCalls'"
     ]
    }
   ],
   "source": [
    "llm_chat_request.is_stream = True\n",
    "step =1 \n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "\n",
    "text,tool_calls= get_tool_calls(chat_response)\n",
    "print(f\"\\n **************************Step {step} Result**************************\")\n",
    "print(f\"message = {text}\")\n",
    "print(f\"tool calls = {tool_calls}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# call tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat_request.tool_results = []\n",
    "for call in tool_calls:\n",
    "    tool_result = oci.generative_ai_inference.models.CohereToolResult()\n",
    "    tool_result.call = call\n",
    "    tool_result.outputs = [ { \"response\": \"Completed\" } ] \n",
    "    llm_chat_request.tool_results.append(tool_result)\n",
    "\n",
    "step = step+1\n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "text,tool_calls= get_tool_calls(chat_response)\n",
    "print(f\"message = {text}\")\n",
    "print(f\"tool calls = {tool_calls}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise : Weather Ansering App\n",
    "\n",
    "1. Create an App that  gives the weather information for a given city \n",
    "    * Eg:  \tA. Lucknow will have temp around 25 degrees and will rain \n",
    "\n",
    "2. calls the weather API for a given city\n",
    "    * Api hosted at : https://ic-edge.ugbu.oraclepdemos.com/ash/docs\n",
    "        * Input \n",
    "            * City Name  \n",
    "            * DatDays in future\n",
    "        * Output\n",
    "            * Low\n",
    "            * High\n",
    "            * Chance of rain \n",
    "3. Types of questions supported\n",
    "    * What is the weather in lucknow tomm\n",
    "    * What will be highs in capital of india\n",
    "    * Which major city in Karnataka will it rain tomm \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
