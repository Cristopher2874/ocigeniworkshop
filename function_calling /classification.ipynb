{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 3 : Function calling \n",
    "\n",
    "## Classifying the question asked into a class of questions\n",
    "- once classfied the appropriate set of tools for that class can be exposed to LLM \n",
    "- This helps tio reducxe the total number of tools exposed to LLM and reduce context data\n",
    "\n",
    "### Supported models (https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm) \n",
    "- cohere.command-r-08-2024\n",
    "- cohere.command-r-16k\n",
    "- cohere.command-r-plus\n",
    "- cohere.command-r-plus-08-2024\n",
    "- meta.llama-3.1-405b-instruct\n",
    "- meta.llama-3.1-70b-instruct\n",
    "- meta.llama-3.2-90b-vision-instruct\n",
    "\n",
    "\n",
    "Questions use #generative-ai-users  or ##igiu-innovation-lab slack channels \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the  variables\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import OnDemandServingMode, EmbedTextDetails,CohereChatRequest, ChatDetails\n",
    "import oci\n",
    "\n",
    "#####\n",
    "#Setup\n",
    "#Change the compartmentid to yhe ocid of your compartment\n",
    "#Change the profile if needed\n",
    "#####\n",
    "\n",
    "CONFIG_PROFILE = \"AISANDBOX\"\n",
    "compartmentId= \"ocid1.compartment.oc1..aaaaaaaaxj6fuodcmai6n6z5yyqif6a36ewfmmovn42red37ml3wxlehjmga\" \n",
    "llm_service_endpoint= \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "llm_client = None\n",
    "llm_payload = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oci key enabled for api access\n",
    "config = oci.config.from_file('~/.oci/config', CONFIG_PROFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up Chat Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cohere_chat_request = CohereChatRequest()\n",
    "cohere_chat_request.preamble_override = \"\"\"\n",
    "        \n",
    "        You are a call classifier you carefully analyze teh question and classify it into one of the following categories. \n",
    "        you then return just the category in response\n",
    "        \n",
    "        categories: billing, outage, program, service\n",
    "        \n",
    "        eg: \n",
    "        question:  can i pay my bill by creditcard\n",
    "        answer: billing\n",
    "        question: i need to cancel my service as i am moving\n",
    "        answer: service\n",
    "        question: when will by power come back on?\n",
    "        answer: outage\n",
    "        \n",
    "        \"\"\"\n",
    "cohere_chat_request.is_stream = False \n",
    "cohere_chat_request.max_tokens = 500\n",
    "cohere_chat_request.temperature = 0.75\n",
    "cohere_chat_request.top_p = 0.7\n",
    "cohere_chat_request.frequency_penalty = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up chat details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_detail = ChatDetails()\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=\"cohere.command-r-plus\")\n",
    "chat_detail.compartment_id = compartmentId\n",
    "chat_detail.chat_request = cohere_chat_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the LLM client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set up the LLM client \n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "                config=config,\n",
    "                service_endpoint=llm_service_endpoint,\n",
    "                retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "                timeout=(10,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_chat_request.message = \"why is my bill so high\"\n",
    "\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "llm_text = llm_response.data.chat_response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** print response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Chat Result**************************\n",
      "billing\n"
     ]
    }
   ],
   "source": [
    "print(\"**************************Chat Result**************************\")\n",
    "#llm_text = llm_response.data.chat_response.text\n",
    "print(llm_response.data.chat_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "\n",
    "Merge your RAG Agent & Single step tool into same code and use clasiification to decide which to invoke\n",
    "eg: \n",
    "1. If the queston is about weather, cll the single step tool\n",
    "2. if the call is about batteries use the RAG to answer\n",
    "3. if its about something else say you can only answer questions about weather or batteries\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
