{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 3 : Agents\n",
    "\n",
    "## Multi Step  Tool function calling\n",
    "\n",
    "### Supported models (https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm) \n",
    "- cohere.command-r-08-2024\n",
    "- cohere.command-r-16k\n",
    "- cohere.command-r-plus\n",
    "- cohere.command-r-plus-08-2024\n",
    "\n",
    "\n",
    " Questions use #generative-ai-users  or #igiu-innovation-lab slack channel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the  variables\n",
    "\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import OnDemandServingMode, EmbedTextDetails,CohereChatRequest, ChatDetails\n",
    "import oci\n",
    "import json\n",
    "\n",
    "CONFIG_PROFILE = \"AISANDBOX\"\n",
    "compartmentId= \"ocid1.compartment.oc1..aaaaaaaaxj6fuodcmai6n6z5yyqif6a36ewfmmovn42red37ml3wxlehjmga\" \n",
    "llm_service_endpoint= \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "LLM_MODEL = \"cohere.command-r-16k\" \n",
    "PREAMBLE = \"\"\"\n",
    "        Analyze teh problem and pick teh right set of tools to answer the question\n",
    "\"\"\"\n",
    "MESSAGE = \"\"\"\n",
    "       Total sales amount over the 28th and 29th of September.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sep up the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report tool \n",
    "date_param = oci.generative_ai_inference.models.CohereParameterDefinition()\n",
    "date_param.description = \"Retrieves sales data for this day, formatted as YYYY-MM-DD.\"\n",
    "date_param.type = \"str\"\n",
    "date_param.is_required = True\n",
    "\n",
    "report_tool = oci.generative_ai_inference.models.CohereTool()\n",
    "report_tool.name = \"query_daily_sales_report\"\n",
    "report_tool.description = \"Connects to a database to retrieve overall sales volumes and sales information for a given day.\"\n",
    "report_tool.parameter_definitions = {\n",
    "    \"date\": date_param\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculator tool \n",
    "\n",
    "expression_param = oci.generative_ai_inference.models.CohereParameterDefinition()\n",
    "expression_param.description = \"The expression to caculate.\"\n",
    "expression_param.type = \"str\"\n",
    "expression_param.is_required = True\n",
    "\n",
    "calculator_tool = oci.generative_ai_inference.models.CohereTool()\n",
    "calculator_tool.name = \"simple_calculator\"\n",
    "calculator_tool.description = \"Connects to a database to retrieve overall sales volumes and sales information for a given day.\"\n",
    "calculator_tool.parameter_definitions = {\n",
    "    \"expression\": expression_param\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## specify the tools to use in the chat request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oci key enabled for api access\n",
    "config = oci.config.from_file('~/.oci/config', CONFIG_PROFILE)\n",
    "\n",
    "# chat request      \n",
    "llm_chat_request = CohereChatRequest()\n",
    "llm_chat_request.preamble_override = PREAMBLE \n",
    "llm_chat_request.message = MESSAGE\n",
    "llm_chat_request.is_stream = False \n",
    "llm_chat_request.max_tokens = 500 # max token to generate, can lead to incomplete responses\n",
    "llm_chat_request.is_force_single_step = False\n",
    "llm_chat_request.tools = [ report_tool, calculator_tool ]\n",
    "\n",
    "\n",
    "\n",
    "# set up chat details\n",
    "chat_detail = ChatDetails()\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=LLM_MODEL)\n",
    "chat_detail.compartment_id = compartmentId\n",
    "chat_detail.chat_request = llm_chat_request\n",
    "\n",
    "# set up the LLM client \n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "                config=config,\n",
    "                service_endpoint=llm_service_endpoint,\n",
    "                retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "                timeout=(10,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## call the LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Step 1 Result**************************\n",
      "message = I will use the query_daily_sales_report tool twice, once for the 28th of September and once for the 29th, and then add the two figures together to find the total sales amount over the two days.\n",
      "tool calls = [{\n",
      "  \"name\": \"query_daily_sales_report\",\n",
      "  \"parameters\": {\n",
      "    \"date\": \"2023-09-28\"\n",
      "  }\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "print(f\"**************************Step {step} Result**************************\")\n",
    "print(f\"message = {chat_response.data.chat_response.text}\")\n",
    "print(f\"tool calls = {chat_response.data.chat_response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the tools \n",
    "\n",
    "Note: \n",
    "1. in this example we are not explicity calling the tool, we are just returning a made up response.  you will insert an explicit call to teh toolapi for real code\n",
    "2. we have to keep calling chat  in a toop, so that llm can look at the tool reponse in generating it response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Step 2 Result**************************\n",
      "message = \n",
      "tool calls = [{\n",
      "  \"name\": \"query_daily_sales_report\",\n",
      "  \"parameters\": {\n",
      "    \"date\": \"2023-09-29\"\n",
      "  }\n",
      "}]\n",
      "**************************Step 3 Result**************************\n",
      "message = The sales amount for the 28th of September is 5000 and the sales amount for the 29th of September is 8000. I will now add these two figures together.\n",
      "tool calls = [{\n",
      "  \"name\": \"simple_calculator\",\n",
      "  \"parameters\": {\n",
      "    \"expression\": \"5000 + 8000\"\n",
      "  }\n",
      "}]\n",
      "**************************Step 4 Result**************************\n",
      "message = The total sales amount over the 28th and 29th of September is **13,000**. The sales amount for the 28th was 5000 and the sales amount for the 29th was 8000.\n",
      "tool calls = None\n"
     ]
    }
   ],
   "source": [
    "tool_results = []\n",
    "llm_chat_request.message = \"\"\n",
    "while chat_response.data.chat_response.tool_calls is not None:   # we have invoke the llm till there are no more tools left \n",
    "    for call in chat_response.data.chat_response.tool_calls: # there amay be more than one tool to call \n",
    "        tool_result = oci.generative_ai_inference.models.CohereToolResult()\n",
    "        tool_result.call = call\n",
    "        if call.name == \"query_daily_sales_report\":\n",
    "            if call.parameters[\"date\"] == \"2023-09-29\":\n",
    "                # We should   call tool here we re simulating teh json response here \n",
    "                tool_result.outputs = [\n",
    "                    {\n",
    "                        \"date\": call.parameters[\"date\"],\n",
    "                        \"summary\": \"Total Sales Amount: 8000, Total Units Sold: 200\"\n",
    "                    }\n",
    "                ] \n",
    "            else:\n",
    "                # We should   call tool here we re simulating teh json response here \n",
    "                tool_result.outputs = [\n",
    "                    {\n",
    "                        \"date\": call.parameters[\"date\"],\n",
    "                        \"summary\": \"Total Sales Amount: 5000, Total Units Sold: 125\"\n",
    "                    }\n",
    "                ] \n",
    "        else:\n",
    "            # We should   call tool here we re simulating teh json response here \n",
    "            tool_result.outputs = [\n",
    "                {\n",
    "                    \"expression\": call.parameters[\"expression\"],\n",
    "                    \"answer\": \"13000\"\n",
    "                }\n",
    "            ]\n",
    "        tool_results.append(tool_result)  # the tool responses are collectec to feed back to the llm \n",
    "\n",
    "    llm_chat_request.chat_history = chat_response.data.chat_response.chat_history\n",
    "    llm_chat_request.tool_results = tool_results\n",
    "    # call the llm again with responses from previous round of tools \n",
    "    step = step +1 \n",
    "    chat_response = llm_client.chat(chat_detail)\n",
    "\n",
    "    # Print result\n",
    "\n",
    "    print(f\"**************************Step {step} Result**************************\")\n",
    "    print(f\"message = {chat_response.data.chat_response.text}\")\n",
    "    print(f\"tool calls = {chat_response.data.chat_response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming version \n",
    "\n",
    "Steaming response reduces latency, specially if response has a lot of text. but its involved as we have to process events \n",
    "\n",
    "we first define the function to process the evnets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_tool_calls_and_chat_history(chat_response):\n",
    "    for event in chat_response.data.events():\n",
    "        res = json.loads(event.data)\n",
    "        text = res['text']\n",
    "        if 'finishReason' in res:\n",
    "            if 'toolCalls' in res:\n",
    "                #print(f\"\\ntools to use : {res['toolCalls']}\",flush=True)\n",
    "                return text,res['toolCalls'], res['chatHistory']\n",
    "            else:\n",
    "                return text,None, res['chatHistory']\n",
    "        else:\n",
    "            if 'text' in res:\n",
    "                print(res['text'], end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### call the llm in streaming mode \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total sales amount over the 28th and 29th of September is **13,000**. The sales amount for the 28th of September was 5000 and the sales amount for the 29th of September was 8000.\n",
      " **************************Step 1 Result**************************\n",
      "message = The total sales amount over the 28th and 29th of September is **13,000**. The sales amount for the 28th of September was 5000 and the sales amount for the 29th of September was 8000.\n",
      "tool calls = None\n"
     ]
    }
   ],
   "source": [
    "llm_chat_request.is_stream = True\n",
    "step =1 \n",
    "chat_response = llm_client.chat(chat_detail)\n",
    "\n",
    "text,tool_calls, chat_history = get_tool_calls_and_chat_history(chat_response)\n",
    "print(f\"\\n **************************Step {step} Result**************************\")\n",
    "print(f\"message = {text}\")\n",
    "print(f\"tool calls = {tool_calls}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# call tools & iterate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_results = []\n",
    "while tool_calls is not None:\n",
    "    for call in tool_calls:\n",
    "        call = oci.generative_ai_inference.models.CohereToolCall(**call)\n",
    "        tool_result = oci.generative_ai_inference.models.CohereToolResult()\n",
    "        tool_result.call = call\n",
    "        if call.name == \"query_daily_sales_report\":\n",
    "            if call.parameters[\"date\"] == \"2023-09-29\":\n",
    "                tool_result.outputs = [\n",
    "                    {\n",
    "                        \"date\": call.parameters[\"date\"],\n",
    "                        \"summary\": \"Total Sales Amount: 8000, Total Units Sold: 200\"\n",
    "                    }\n",
    "                ] \n",
    "            else:\n",
    "                tool_result.outputs = [\n",
    "                    {\n",
    "                        \"date\": call.parameters[\"date\"],\n",
    "                        \"summary\": \"Total Sales Amount: 5000, Total Units Sold: 125\"\n",
    "                    }\n",
    "                ] \n",
    "        else:\n",
    "            tool_result.outputs = [\n",
    "                {\n",
    "                    \"expression\": call.parameters[\"expression\"],\n",
    "                    \"answer\": \"13000\"\n",
    "                }\n",
    "            ]\n",
    "        tool_results.append(tool_result)\n",
    "\n",
    "    llm_chat_request.chat_history = chat_history\n",
    "    #llm_chat_request.tool_results = tool_results\n",
    "    step = step+1\n",
    "    chat_response = llm_client.chat(chat_detail)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"\\n**************************Step {step} Result**************************\",flush=True)\n",
    "    #print(vars(chat_response))\n",
    "    text,tool_calls, chat_history = get_tool_calls_and_chat_history(chat_response)\n",
    "    print(f\"message = {text}\")\n",
    "    print(f\"tool calls = {tool_calls}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Clothes Recommender app \n",
    "Create an App that  Answers the Question : \n",
    "    * What clothes should I wear to Oracles headquarter tomm\n",
    "\n",
    "1. Use following APis : https://ic-edge.ugbu.oraclepdemos.com/ash/docs\n",
    "    * Weather API\n",
    "    * City API\n",
    "    * Clothes API\n",
    "1. Things to try \n",
    "    * Conversational History/ Documents for part of information eg: gender / location etc\n",
    "    * Context in preamble\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
