{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the compartment and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "import base64\n",
    "\n",
    "\n",
    "CONFIG_PROFILE = \"AISANDBOX\"\n",
    "compartmentId= \"ocid1.compartment.oc1..aaaaaaaaxj6fuodcmai6n6z5yyqif6a36ewfmmovn42red37ml3wxlehjmga\" \n",
    "llm_service_endpoint= \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "llm_client = None\n",
    "llm_payload = None\n",
    "image_path = \"dussera-b.jpg\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def get_message():\n",
    "        content1 = oci.generative_ai_inference.models.TextContent()\n",
    "        content1.text = \"tell me this image\"\n",
    "        content2 = oci.generative_ai_inference.models.ImageContent()\n",
    "        image_url = oci.generative_ai_inference.models.ImageUrl()\n",
    "        image_url.url = f\"data:image/jpeg;base64,{encode_image(image_path)}\"\n",
    "        content2.image_url = image_url\n",
    "        message = oci.generative_ai_inference.models.UserMessage()\n",
    "        message.content = [content1,content2] \n",
    "\n",
    "        return message\n",
    "\n",
    "def get_chat_request():\n",
    "        chat_request = oci.generative_ai_inference.models.GenericChatRequest()\n",
    "        #chat_request.preamble_override = \"you always answer in a one stanza poem.\"\n",
    "        #chat_request.message = get_message()\n",
    "        chat_request.messages = [get_message()]\n",
    "        chat_request.api_format = oci.generative_ai_inference.models.BaseChatRequest.API_FORMAT_GENERIC\n",
    "        chat_request.num_generations = 1\n",
    "        chat_request.is_stream = False \n",
    "        chat_request.max_tokens = 500\n",
    "        chat_request.temperature = 0.75\n",
    "        chat_request.top_p = 0.7\n",
    "        chat_request.top_k = -1 \n",
    "        chat_request.frequency_penalty = 1.0\n",
    "\n",
    "        return chat_request\n",
    "\n",
    "def get_chat_detail (llm_request):\n",
    "        chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "        chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=\"meta.llama-3.2-90b-vision-instruct\")\n",
    "        chat_detail.compartment_id = compartmentId\n",
    "        chat_detail.chat_request = llm_request\n",
    "\n",
    "        return chat_detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the keys and instantiate the client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = oci.config.from_file('~/.oci/config', CONFIG_PROFILE)\n",
    "\n",
    "llm_client = oci.generative_ai_inference.GenerativeAiInferenceClient(\n",
    "                config=config,\n",
    "                service_endpoint=llm_service_endpoint,\n",
    "                retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "                timeout=(10,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the payload and call the client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_payload =get_chat_detail(get_chat_request())\n",
    "\n",
    "llm_response = llm_client.chat(llm_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_text = llm_response.data.chat_response.choices[0].message.content[0].text\n",
    "print (llm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : Document Validator\n",
    "\n",
    "1. Create a document in PowerPoint with\n",
    "    * Name\n",
    "    * Address\n",
    "    * Dates \n",
    "    * Create date\n",
    "    * Expiry date\n",
    "    * Signature\n",
    "\n",
    "1. Save it as an image \n",
    "\n",
    "1. Use Document Understanding & LLM service to validate\n",
    "    *  is on correct name\n",
    "    *  is on correct address\n",
    "    *  is not expired\n",
    "    *  has a signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 : Form Filler\n",
    "\n",
    "1. Upload a receipt \n",
    "1. Fill out a expense report based on the receipt \n",
    "    *  Image an expense resporr as an multi line f string for simplicity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
