{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 2 : RAG\n",
    "\n",
    "This is an example for all steps in a basic RAG solution \n",
    "steps: \n",
    "\n",
    "1. Setup :  chunk/embed/store\n",
    "2. R-etrival :  embed & do similarity search\n",
    "3. A -ugmnet :  optionally rerank & add to response\n",
    "4. G - enerate : ask LLm to answer question based on the retrieved chunks\n",
    "\n",
    "Questions use #generative-ai-users  or #igiu-innovation-lab slack channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import OnDemandServingMode, EmbedTextDetails,CohereChatRequest, ChatDetails\n",
    "import oracledb\n",
    "import array\n",
    "import oci\n",
    "import os,json \n",
    "\n",
    "\n",
    "#####\n",
    "#make sure your sandbox.json file is setup for your environment. You might have to specify the full path depending on  your `cwd` \n",
    "#####\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.json\"\n",
    "\n",
    "EMBED_MODEL = \"cohere.embed-multilingual-v3.0\"\n",
    "LLM_MODEL = \"cohere.command-r-08-2024\" \n",
    "llm_service_endpoint= \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "tablename_prefix = None\n",
    "compartmentId = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunks we we want to query against "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# here we are starting with samll chunks. Idelaly you will have to parse teh file and chunk it using a library\n",
    "# there are quite a few strategies on parsing and chunking. do you wn ownresearch and enahcne this code. \n",
    "chunks = [\n",
    "    \t\t\t\"Baseball is a great game \",\n",
    "\t\t\t\t\"baseball games typically last 9 innings\",\n",
    "\t\t\t\t\"Baseball game can finish in about 2 hours\",\n",
    "\t\t\t\t\"Indias favroite passtime is cricket\",\n",
    "\t\t\t\t\"England's favorite passtime is football\",\n",
    "\t\t\t\t\"Football is called soccer in America\",\n",
    "\t\t\t\t\"baseball is americas favroite pass time sport\"]\n",
    "\n",
    "# we are mocking the cource of chunks. this will be used in citations. This helps build confidence, avoid hallucination. \n",
    "chunk_source = [ \n",
    "                {\"chapter\":\"Baseball\", \"question\":\"1\"},\n",
    "                {\"chapter\":\"Baseball\", \"question\":\"2\"},\n",
    "                {\"chapter\":\"Baseball\", \"question\":\"3\"},\n",
    "                {\"chapter\":\"Cricket\", \"question\":\"1\"},\n",
    "                {\"chapter\":\"Football\", \"question\":\"1\"},\n",
    "                {\"chapter\":\"Football\", \"question\":\"2\"},\n",
    "                {\"chapter\":\"Baseball\", \"question\":\"4\"},\n",
    "\t\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scfg = None\n",
    "# read the sandbox config \n",
    "with open(os.path.expanduser(SANDBOX_CONFIG_FILE), 'r') as f:\n",
    "                scfg=  json.load(f)\n",
    "                \n",
    "config = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]),scfg[\"oci\"][\"profile\"])\n",
    "compartmentId = scfg[\"oci\"][\"compartment\"]\n",
    "tablename_prefix = scfg[\"db\"][\"tablePrefix\"]\n",
    "wallet = os.path.expanduser(scfg[\"db\"][\"walletPath\"])\n",
    "                \n",
    "db = oracledb.connect(  config_dir=scfg[\"db\"][\"walletPath\"],user= scfg[\"db\"][\"username\"], password=scfg[\"db\"][\"password\"], dsn=scfg[\"db\"][\"dsn\"],wallet_location=scfg[\"db\"][\"walletPath\"],wallet_password=scfg[\"db\"][\"walletPass\"])\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = [\n",
    "\t\tf\"\"\"drop table if exists {tablename_prefix}_embedding purge\"\"\"\t,\n",
    "  \n",
    "\t\tf\"\"\"\n",
    " \t\tcreate table {tablename_prefix}_embedding (\n",
    "   \t\t\tid number,\n",
    "\t\t\ttext varchar2(4000),\n",
    "\t\t\tvec vector,\n",
    "\t\t\tchapter varchar2(100),\n",
    "\t\t\tsection integer,\n",
    "\t\t\tprimary key (id)\n",
    "\t\t)\"\"\"\n",
    "\t]\n",
    " \n",
    "for s in sql : \n",
    "\t\tcursor.execute(s)\n",
    "\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up LLM client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create a llm client \n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "\t\t\t\tconfig=config, \n",
    "\t\t\t\tservice_endpoint=llm_service_endpoint, \n",
    "\t\t\t\tretry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "\t\t\t\ttimeout=(10,240))\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_text_detail = EmbedTextDetails()\n",
    "embed_text_detail.serving_mode = OnDemandServingMode(model_id=EMBED_MODEL)\n",
    "embed_text_detail.truncate = embed_text_detail.TRUNCATE_END\n",
    "embed_text_detail.input_type = EmbedTextDetails.INPUT_TYPE_SEARCH_DOCUMENT \n",
    "embed_text_detail.compartment_id = compartmentId\n",
    "embed_text_detail.inputs = chunks\n",
    "\n",
    "response = llm_client.embed_text(embed_text_detail)\n",
    "embeddings = response.data.embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insert embedding in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(embeddings)):\n",
    "    cursor.execute(f\"insert into {tablename_prefix}_embedding values (:1, :2, :3, :4, :5)\", \n",
    "                   [i, chunks[i], array.array(\"f\",embeddings[i]),chunk_source[i][\"chapter\"], chunk_source[i][\"question\"] ])\n",
    "    print(f\"inserted {i}-{chunks[i]}\")\n",
    "\n",
    "print(\"commiting\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"select id,text from {tablename_prefix}_embedding\")\n",
    "for row in cursor:\n",
    "\tprint(f\"{row[0]}:{row[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask A question to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Ask a question: \").strip().lower()\n",
    "q=[]\n",
    "q.append(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embed the query\n",
    "\n",
    "we nede to do the \"R\" part of rag - retrieve.  we retrieve in following steps\n",
    "1. embed the query test\n",
    "1. do a similarity serach to find the text similar to it \n",
    "2. optionally rerank it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embed\n",
    "\n",
    "embed_text_detail.inputs = q\n",
    "embed_text_detail.input_type = EmbedTextDetails.INPUT_TYPE_SEARCH_QUERY\n",
    "response = llm_client.embed_text(embed_text_detail)\n",
    "vec = array.array(\"f\",response.data.embeddings[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simialrity search of embedded text \n",
    " \n",
    "# There are multiple search algorithms: COSINE, DOT, EUCLIDEAN\n",
    "cursor.execute(f\"\"\"\n",
    "\t\tselect id,text, vector_distance(vec, :1, COSINE) d, chapter,section \n",
    "\t\tfrom {tablename_prefix}_embedding\n",
    "\t\torder by d\n",
    "\t\tfetch first 3 rows only\n",
    "\t\"\"\", [vec])\n",
    "\n",
    "rows =[]\n",
    "for row in cursor:\n",
    "\tr = [row[0], row[1], row[2], f\"chapter:[{row[3]}]_section:[{row[4]}]\"]\n",
    "\tprint(r)\n",
    "\trows.append(r)\n",
    "\n",
    "\n",
    "print (rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optionally rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at cohere reranking example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A of RAG : augment\n",
    "\n",
    "we attach the retrieved chucks as documents to chat request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the payloafd \n",
    "# chat request \n",
    "cohere_chat_request = CohereChatRequest()\n",
    "#cohere_chat_request.preamble_override = \"you always answer in a one stanza poem.\"\n",
    "cohere_chat_request.is_stream = False \n",
    "cohere_chat_request.max_tokens = 500\n",
    "cohere_chat_request.temperature = 0.75\n",
    "cohere_chat_request.top_p = 0.7\n",
    "cohere_chat_request.frequency_penalty = 1.0\n",
    "#cohere_chat_request.documents = get_documents()\n",
    "\n",
    "#chat detail \n",
    "chat_detail = ChatDetails()\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=\"cohere.command-r-plus\")\n",
    "chat_detail.compartment_id = compartmentId\n",
    "chat_detail.chat_request = cohere_chat_request\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add documents\n",
    "docs =[]\n",
    "for chunk in rows:\n",
    "    print (chunk)\n",
    "    doc = {\n",
    "        \"id\" : chunk[3],\n",
    "       \"snippet\" : chunk[1]\n",
    "    } \n",
    "    docs.append(doc)\n",
    "\n",
    "cohere_chat_request.documents = docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G in RAG : Generate the respone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_chat_request.message = query\n",
    "cohere_chat_request.preamble_override = \" answer only from selected docs, ignore any other information you may know\"\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "print (\"query executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print the response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"**************************Chat Result**************************\")\n",
    "print(query)\n",
    "llm_text = llm_response.data.chat_response.text\n",
    "print(llm_response.data.chat_response.text)\n",
    "print(\"************************** Citations**************************\")\n",
    "print(llm_response.data.chat_response.citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## close the dabasae connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "db.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    " * Implement a “talk-to-document”\n",
    " * Try one of :\n",
    "    * Text document\n",
    "    * PDF document\n",
    " * Play with following features\n",
    "    * Chunking\n",
    "    * Different types similarity search\n",
    "    * Reranking\n",
    "    * Citations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
