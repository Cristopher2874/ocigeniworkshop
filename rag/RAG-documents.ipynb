{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 2 : RAG\n",
    "\n",
    "## Passing documents & citations\n",
    "\n",
    "### Supported models (https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm) \n",
    "- cohere.command-r-08-2024\n",
    "- cohere.command-r-16k\n",
    "- cohere.command-r-plus\n",
    "- cohere.command-r-plus-08-2024\n",
    "- meta.llama-3.1-405b-instruct\n",
    "- meta.llama-3.1-70b-instruct\n",
    "- meta.llama-3.2-90b-vision-instruct\n",
    "  \n",
    "Questions use #generative-ai-users  or ##igiu-innovation-lab slack channels \n",
    "if you have errors running sample code reach out for help in #igiu-ai-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the  variables\n",
    "\n",
    "\n",
    "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import *\n",
    "import oci\n",
    "import os,json\n",
    "\n",
    "#####\n",
    "#make sure your sandbox.json file is setup for your environment. You might have to specify the full path depending on  your `cwd` \n",
    "#####\n",
    "SANDBOX_CONFIG_FILE = \"sandbox.json\"\n",
    "\n",
    "LLM_MODEL = \"cohere.command-r-plus-08-2024\" \n",
    "llm_service_endpoint= \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "llm_client = None\n",
    "llm_payload = None\n",
    "\n",
    "\n",
    "PREAMBLE = \"provide factual answers based of document provided nclude citations if you can. Say you cant answer if the answer is not in provided documents \"\n",
    "MESSAGE = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scfg = None\n",
    "# read the sandbox config \n",
    "with open(os.path.expanduser(SANDBOX_CONFIG_FILE), 'r') as f:\n",
    "                scfg=  json.load(f)\n",
    "\n",
    "#read the oci config\n",
    "config = oci.config.from_file(os.path.expanduser(scfg[\"oci\"][\"configFile\"]),scfg[\"oci\"][\"profile\"])\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up Chat Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# chat request      \n",
    "llm_chat_request = CohereChatRequest()\n",
    "#llm_chat_request.preamble_override = PREAMBLE \n",
    "llm_chat_request.message = MESSAGE\n",
    "llm_chat_request.is_stream = False \n",
    "llm_chat_request.max_tokens = 500 # max token to generate, can lead to incomplete responses\n",
    "llm_chat_request.temperature = 1.0 # higer value menas more randon, defaul = 0.3\n",
    "#llm_chat_request.seed = 7555 # makes the best effort to make answer determininstic , not gaureented \n",
    "llm_chat_request.top_p = 0.7  # ensures only tokens with toptal probabely of p are considered, max value = 0.99, min 0.01, default 0.75\n",
    "llm_chat_request.top_k = 0  #Ensures that only top k tokens are considered, 0 turns it off, max = 500\n",
    "llm_chat_request.frequency_penalty = 0.0 # reduces the repeatedness of tokens max value 1.9=0, min 0,0\n",
    "#cohere_chat_request.documents = get_documents()  # will only answer from supplied documnets not frm its own knopwledge\n",
    "cohere_chat_request.citation_quality =  CohereChatRequest.CITATION_QUALITY_FAST # FAST or accurate\n",
    "#cohere_chat_request.citation_quality =  cohere_chat_request.CITATION_QUALITY_ACCURATE # FAST or accurate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "                 {\n",
    "                        \"title\": \"Oracle\",\n",
    "                        \"snippet\": \"Oracle database services and products offer customers cost-optimized and high-performance versions of Oracle Database, the world's leading converged, multi-model database management system, as well as in-memory, NoSQL and MySQL databases. Oracle Autonomous Database, available on premises via Oracle Cloud@Customer or in the Oracle Cloud Infrastructure, enables customers to simplify relational database environments and reduce management workloads.\",\n",
    "                        \"website\": \"https://www.oracle.com/database\",\n",
    "                        \"id\": \"ORA001\"\n",
    "                },\n",
    "                 {\n",
    "                        \"title\": \"Amazon\",\n",
    "                        \"snippet\": \"\"\" AWS provides the broadest selection of purpose-built databases allowing you to save, grow, and innovate faster.\n",
    "Purpose Built\n",
    "Choose from 15+ purpose-built database engines including relational, key-value, document, in-memory, graph, time series, wide column, and ledger databases.\n",
    "Performance at Scale\n",
    "Get relational databases that are 3-5X faster than popular alternatives, or non-relational databases that give you microsecond to sub-millisecond latency.\n",
    "Fully Managed\n",
    "AWS continuously monitors your clusters to keep your workloads running with self-healing storage and automated scaling, so that you can focus on application development.\n",
    "Secure & Highly Available\n",
    "AWS databases are built for business-critical, enterprise workloads, offering high availability, reliability, and security.\n",
    "\"\"\",\n",
    "                        \"website\": \"https://aws.amazon.com/free/database/e\",\n",
    "                        \"id\": \"AWS001\"\n",
    "                }\n",
    "]\n",
    "llm_chat_request.documents = docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up chat details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# set up chat details\n",
    "chat_detail = ChatDetails()\n",
    "chat_detail.serving_mode = OnDemandServingMode(model_id=LLM_MODEL)\n",
    "chat_detail.compartment_id =scfg[\"oci\"][\"compartment\"] \n",
    "chat_detail.chat_request = llm_chat_request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the LLM client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set up the LLM client \n",
    "llm_client = GenerativeAiInferenceClient(\n",
    "                config=config,\n",
    "                service_endpoint=llm_service_endpoint,\n",
    "                retry_strategy=oci.retry.NoneRetryStrategy(),\n",
    "                timeout=(10,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm_chat_request.seed = 7555 # trting changing to see if we can reproduce teh opriginal response\n",
    "llm_chat_request.message = \"tell me more about AWS\"\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "llm_text = llm_response.data.chat_response.text\n",
    "        \n",
    "print(\"**************************Chat Result**************************\")\n",
    "#llm_text = llm_response.data.chat_response.text\n",
    "print(llm_response.data.chat_response.text)\n",
    "print(\"************************** Citations**************************\")\n",
    "print(llm_response.data.chat_response.citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update the history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update history \n",
    "previous_chat_message = oci.generative_ai_inference.models.CohereUserMessage(message=\"Tell me something about Oracle.\")\n",
    "previous_chat_reply = oci.generative_ai_inference.models.CohereChatBotMessage(message=\"Oracle is one of the largest vendors in the enterprise IT market and the shorthand name of its flagship product. The database software sits at the center of many corporate IT\")\n",
    "        \n",
    "llm_chat_request.chat_history =  [previous_chat_message, previous_chat_reply]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ask the question again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm_chat_request.documents = []\n",
    "llm_chat_request.message = \"tell me more about its databases\"\n",
    "llm_response = llm_client.chat(chat_detail)\n",
    "llm_text = llm_response.data.chat_response.text\n",
    "        \n",
    "print (llm_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** print response & citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"**************************Chat Result**************************\")\n",
    "#llm_text = llm_response.data.chat_response.text\n",
    "print(llm_response.data.chat_response.text)\n",
    "print(\"************************** Citations**************************\")\n",
    "print(llm_response.data.chat_response.citations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
